{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSS 709 Bible Translation Analysis\n",
    "--------------------------------------------------\n",
    "\n",
    "This notebook is used to generate word2vec distances for certain concepts within bible translations, as part of CSS 709 (Natural Language Processing) course. The input consists of existing bible texts, typically in .txt format. The output consists of an array containing the differences in distance between words \"Holy\" and \"Evil\" for an array of concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Max\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code defines the utility functions necessary to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to import and preprocess non-UTF encoded text\n",
    "def data_import(fname):\n",
    "    # Import text file and clean it\n",
    "    with open(fname, newline = '') as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "    data = [re.sub(r'<.*?>', '', word) for word in data] # remove HTML tags - just in case some are missed.\n",
    "    data = [re.sub(r'’s|\\'s', '', word) for word in data] # remove possessive \"s\"\n",
    "    data = [re.sub(r'\\n|\\\\\\\\t', '', word) for word in data] # remove line breaks, tab breaks\n",
    "    data = [re.sub(r'[^\\w\\s]|_', '', word) for word in data] # remove punctuation and underscore\n",
    "    data = [re.sub(r'\\w*\\d\\w*', '', word) for word in data] # remove character strings that contain a digit\n",
    "    data = [re.sub(r'\\d', '', word) for word in data] # remove digits  \n",
    "    data = [word.lower() for word in data] # convert the text to lowercase\n",
    "    data = [word.split() for word in data] # split the sentences into words        \n",
    "    data = [sent for sent in data if sent != []] # remove empty tokens\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# This function is used to import and  preprocess UTF encoded text\n",
    "def data_importUTF(fname):\n",
    "    # Import text file and clean it. This version is used for UTF encoding\n",
    "    with open(fname, encoding=\"utf8\", newline = '') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "\n",
    "    data = [re.sub(r'<.*?>', '', word) for word in data] # remove HTML tags - just in case some are missed.\n",
    "    data = [re.sub(r'’s|\\'s', '', word) for word in data] # remove possessive \"s\"\n",
    "    data = [re.sub(r'\\n|\\\\\\\\t', '', word) for word in data] # remove line breaks, tab breaks\n",
    "    data = [re.sub(r'[^\\w\\s]|_', '', word) for word in data] # remove punctuation and underscore\n",
    "    data = [re.sub(r'\\w*\\d\\w*', '', word) for word in data] # remove character strings that contain a digit\n",
    "    data = [re.sub(r'\\d', '', word) for word in data] # remove digits \n",
    "    data = [word.lower() for word in data] # convert the text to lowercase\n",
    "    data = [word.split() for word in data] # split the sentences into words     \n",
    "    data = [sent for sent in data if sent != []] # remove empty tokens\n",
    "        \n",
    "    return data\n",
    "\n",
    "# this function calculates the sentiment value for each concept passed into the function\n",
    "# based on the two words representing positive and negative sentiments.\n",
    "def distance_array(concepts, goodword, badword, model):\n",
    "    temp_array = []\n",
    "    # Calculate the distance between each concept and the good/bad words, capturing the values in an array.\n",
    "    for i in range(len(concepts)):\n",
    "        temp_array.append(model.wv.similarity(concepts[i], goodword) - model.wv.similarity(concepts[i], badword))\n",
    "    return temp_array\n",
    "\n",
    "# convert sentences to words and remove accents.\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=de-accentize\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# remove stopwords based on the list of stopwords       \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to calculate output loss to evaluate how many epochs should be used. \n",
    "It is not used when running the final script and is commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass LossLogger(CallbackAny2Vec):\\n    # Output loss at each epoch\\n    def __init__(self):\\n        self.epoch = 1\\n        self.losses = [0]\\n\\n    def on_epoch_begin(self, model):\\n        #print(f'Epoch: {self.epoch}', end='\\t')\\n\\n    def on_epoch_end(self, model):\\n        loss = model.get_latest_training_loss()\\n        lossDelta = loss - self.losses[self.epoch - 1]\\n        self.losses.append(loss)\\n        #print(f'  Loss: {lossDelta}')\\n        self.epoch += 1\\n\\nloss_logger = LossLogger()\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class LossLogger(CallbackAny2Vec):\n",
    "    # Output loss at each epoch\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.losses = [0]\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        #print(f'Epoch: {self.epoch}', end='\\t')\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        lossDelta = loss - self.losses[self.epoch - 1]\n",
    "        self.losses.append(loss)\n",
    "        #print(f'  Loss: {lossDelta}')\n",
    "        self.epoch += 1\n",
    "\n",
    "loss_logger = LossLogger()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code calculates all relevant statistics for the input Bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is where we can specify the input text.\n",
    "fname = 'WEB.txt'\n",
    "\n",
    "# Depending on the file type, the appropriate data import function is used\n",
    "#data = data_import(fname)\n",
    "data = data_importUTF(fname)\n",
    "\n",
    "# this section defines the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# adding custom stopwords, mainly from medieval english\n",
    "stop_words.extend(['shall', 'unto', 'thou', 'thy', 'ye', 'thee', 'upon', 'shalt', 'hath', 'also', 'us', 'hast', 'thine'])\n",
    "\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "# Define concepts to be examined\n",
    "concepts = ['man', 'woman', 'angel', 'devil', 'money', 'bread', 'wine', 'home', 'sword',\n",
    "            'soldier', 'goat', 'priest', 'wilderness', 'servant', 'king', 'egypt',\n",
    "            'jerusalem', 'babylon', 'family', 'knowledge']\n",
    "\n",
    "# if the concept words need to be changed, the list below will be used as a reference when changing the words back.\n",
    "'''\n",
    "concepts = ['man', 'woman', 'angel', 'devil', 'money', 'bread', 'wine', 'home', 'sword',\n",
    "            'soldier', 'goat', 'priest', 'wilderness', 'servant', 'king', 'egypt',\n",
    "            'jerusalem', 'babylon', 'family', 'knowledge']\n",
    "            '''\n",
    "\n",
    "# define the two words representing the positive and negative dimension\n",
    "good_word = 'holy'\n",
    "bad_word = 'evil'\n",
    "\n",
    "# initialize the list that will contain our results\n",
    "distance_matrix = []\n",
    "\n",
    "# model params:\n",
    "CONTEXT_WINDOW = 5\n",
    "NEGATIVES = 15\n",
    "MIN_COUNT = 1\n",
    "EPOCHS = 20\n",
    "\n",
    "# create the word2vec model 10 times, each time recording the sentiment values and storing them.\n",
    "for i in range(10):\n",
    "    bible_model = gensim.models.Word2Vec(sentences = data_words,\n",
    "                                         workers = 4,\n",
    "                                         min_count = MIN_COUNT,\n",
    "                                         negative = NEGATIVES,\n",
    "                                         window = CONTEXT_WINDOW,\n",
    "                                         vector_size = 100,\n",
    "                                         sg = 0,\n",
    "                                         #callbacks = [loss_logger],\n",
    "                                         #compute_loss = True,\n",
    "                                         epochs = EPOCHS) \n",
    "    \n",
    "    #print(bible_model.wv.most_similar('evil'))\n",
    "    #print(bible_model.wv.most_similar('angel'))\n",
    "    #print(bible_model.wv.similarity('holy', 'evil'))\n",
    "\n",
    "    # update the result matrix for each model run\n",
    "    distance_matrix.append(distance_array(concepts, good_word, bad_word, bible_model))\n",
    "\n",
    "# convert the result matrix into an numpy matrix\n",
    "np_matrix = np.array(distance_matrix)\n",
    "\n",
    "# use the numpy mean function to find the mean values for sentiments\n",
    "final_output = np.mean(np_matrix, axis = 0)\n",
    "\n",
    "# print out the results to be copied into excel spreadsheet.\n",
    "for i in range (len(final_output)):\n",
    "    print(final_output[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
